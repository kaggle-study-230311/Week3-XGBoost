{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcc0d78-a92f-4fea-a11e-1d158a51f3ad",
   "metadata": {},
   "source": [
    "# LGMB with random split for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7dbf89-1df0-4646-b76a-5b198ae7424e",
   "metadata": {},
   "source": [
    "해당 노브툭은 https://www.kaggle.com/code/mlisovyi/feature-engineering-lighgbm-with-f1-macro/notebook 을 fork 하여 다음 사항들을 개선하였다.\n",
    "* LightGBM 모델이 XGBoost로 교체되었으며 그에 따라 코드가 업데이트되었습니다.\n",
    "* 또한 랜덤포레스트의 투표분류기를 장착하고 XGB의 결과를 RF와 조합하고 있습니다.\n",
    "* 몇 가지 추가 기능이 추가되었습니다.\n",
    "* 이전에 삭제되었던 일부 기능은 그대로 유지되었습니다.\n",
    "* 일부 코드가 재구성되었습니다.\n",
    "* 데이터를 한 번 분할하여 LGBM 조기 정지를 위한 검증 데이터로 사용하는 대신, 훈련 중에 데이터를 분할하여 전체 훈련 집합을 훈련할 수 있도록 했습니다. 이 경우 K-배 분할보다 이 방법이 더 효과적이라는 것을 알게 되었습니다.\n",
    "\n",
    "일부 추가 기능은 https://www.kaggle.com/code/kuriyaman1002/reduce-features-140-85-keeping-f1-score?scriptVersionId=4854294 노트북에서 가져왔다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994988c-521d-4e61-9abe-68a67388f37f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notes from Original Kernel (edited by EAS):\n",
    "\n",
    "해당 커널은 https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro 을 따르지만 하이퍼파라미터 최적화 대신 커널 최적값을 사용해 더 빠르게 실행된다.\n",
    "\n",
    "몇 가지 핵심 사항:\n",
    "\n",
    "* 이 커널은 가구에 대한 집계를 추출한 후 가구주에 대해서만 훈련을 실행합니다. 이는 공지된 채점 시작 방법을 따릅니다: 채점에는 세대주만 사용된다는 점에 유의하세요. 모든 가구 구성원은 시험 + 샘플 제출에 포함되지만 가구주만 채점됩니다. (데이터 설명에서). 그러나 현재로서는 세대주가 아닌 가구원도 평가에 포함되는 것으로 보입니다(https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115 참조). 실제로, 완전 예측은 ~ 0.4 PLB 점수를 제공하는 반면, 모든 비 세대주 항목을 1 등급으로 대체하면 ~ 0.2 PLB 점수로 떨어집니다.\n",
    "* 클래스 주파수의 균형을 맞추는 것이 매우 중요한 것 같습니다. 밸런싱을 하지 않고 훈련된 모델은 ~0.39 PLB / ~0.43 로컬 테스트를 제공하는 반면, 밸런싱을 추가하면 ~0.42 PLB / 0.47 로컬 테스트를 제공합니다. 밸런싱은 수작업으로 할 수도 있고 언더샘플링을 통해 달성할 수도 있습니다. 그러나 가장 간단하고 언더샘플링에 비해 더 강력한 방법은 sklearn API의 LightGBM 모델 생성자에서 class_weight='balanced'를 설정하는 것입니다.\n",
    "* 이 커널은 매크로 F1 점수를 사용하여 훈련에서 조기 중단합니다. 이는 채점 전략에 맞추기 위해 수행됩니다.\n",
    "* 범주형은 블라인드 레이블 인코딩 대신 적절한 매핑을 통해 숫자로 변환됩니다.\n",
    "* 레이블 인코딩으로 바꾸면 트리 모델에서 더 쉽게 소화할 수 있으므로 OHE를 사용합니다. 이 트릭은 트리가 아닌 모델에는 해로울 수 있으므로 주의하세요.\n",
    "* idhogar는 훈련에 사용되지 않습니다. 정보가 유출될 수 있는 유일한 방법은 데이터 유출이 있을 때뿐입니다. 우리는 여기서 빈곤과 싸우고 있습니다. 유출을 악용한다고 해서 빈곤이 줄어들지는 않습니다.)\n",
    "* 가구 내에서 집계가 이루어지며 새로운 기능은 수작업으로 만들어집니다. 대부분의 기능이 이미 가구 단위로 집계되어 있기 때문에 집계할 수 있는 기능이 많지 않다는 점에 유의하세요.\n",
    "* 투표 분류기는 여러 LightGBM 모델에 대한 평균을 내기 위해 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8056abda-83cf-4080-9f88-a3ece5c416bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:39.786784Z",
     "iopub.status.busy": "2023-03-24T23:29:39.786267Z",
     "iopub.status.idle": "2023-03-24T23:29:41.105195Z",
     "shell.execute_reply": "2023-03-24T23:29:41.104838Z",
     "shell.execute_reply.started": "2023-03-24T23:29:39.786747Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "# from sklearn.externals.joblib import Parallel, delayed\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de58ef6-7176-4a60-bf5d-60a741050dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.106271Z",
     "iopub.status.busy": "2023-03-24T23:29:41.106089Z",
     "iopub.status.idle": "2023-03-24T23:29:41.109222Z",
     "shell.execute_reply": "2023-03-24T23:29:41.108966Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.106262Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# this only transforms the idhogar field, the other things this function used to do are done elsewhere\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "\n",
    "# plot feature importance for sklearn decision trees    \n",
    "def feature_importance(forest, X_train, display_results=True):\n",
    "    ranked_list = []\n",
    "    zero_features = []\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    if display_results:\n",
    "        # Print the feature ranking\n",
    "        print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        if display_results:\n",
    "            print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n",
    "        \n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "        \n",
    "        if importances[indices[f]] == 0.0:\n",
    "            zero_features.append(X_train.columns[indices[f]])\n",
    "            \n",
    "    return ranked_list, zero_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676aab81-710d-48f2-8815-18cf6a565827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.109682Z",
     "iopub.status.busy": "2023-03-24T23:29:41.109596Z",
     "iopub.status.idle": "2023-03-24T23:29:41.113566Z",
     "shell.execute_reply": "2023-03-24T23:29:41.113333Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.109674Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n",
    "                ]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "\n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "    \n",
    "    # aggregation rules over household\n",
    "    aggs_num = {'age': ['min', 'max', 'mean'],\n",
    "                'escolari': ['min', 'max', 'mean']\n",
    "               }\n",
    "    \n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean', 'count']\n",
    "\n",
    "    # aggregation over household\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "\n",
    "    # Drop id's\n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fd59046-ed94-4dfb-8434-9b12ca6802f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.114034Z",
     "iopub.status.busy": "2023-03-24T23:29:41.113956Z",
     "iopub.status.idle": "2023-03-24T23:29:41.117589Z",
     "shell.execute_reply": "2023-03-24T23:29:41.117315Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.114026Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert one hot encoded fields to label encoding\n",
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n",
    "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n",
    "               'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        #deal with those OHE, where there is a sum over columns == 0\n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n",
    "                  .format(s_))\n",
    "            # dummy colmn name to be added\n",
    "            col_dummy = s_+'_dummy'\n",
    "            # add the column to the dataframe\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            # add the name to the list of columns to be label-encoded\n",
    "            cols_s_.append(col_dummy)\n",
    "            # proof-check, that now the category is complete\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                 print(\"The category completion did not work\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2199b-bbbc-456c-9c9d-4e5595a7c129",
   "metadata": {},
   "source": [
    "# Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0fd7db-eaee-4480-8e05-0b8ce4a28b22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.118947Z",
     "iopub.status.busy": "2023-03-24T23:29:41.118843Z",
     "iopub.status.idle": "2023-03-24T23:29:41.253957Z",
     "shell.execute_reply": "2023-03-24T23:29:41.253544Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.118939Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/Costa Rican Household Poverty Level Prediction/train.csv')\n",
    "test = pd.read_csv('../input/Costa Rican Household Poverty Level Prediction/test.csv')\n",
    "\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5098dc88-73b5-4d1e-a24a-4fa62e98c64c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.254523Z",
     "iopub.status.busy": "2023-03-24T23:29:41.254423Z",
     "iopub.status.idle": "2023-03-24T23:29:41.348849Z",
     "shell.execute_reply": "2023-03-24T23:29:41.348334Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.254513Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    # encode the idhogar\n",
    "    encode_data(df_)\n",
    "    \n",
    "    # create aggregate features\n",
    "    return do_features(df_)\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6413a",
   "metadata": {},
   "source": [
    "일부 누락된 데이터를 정리하고 개체를 숫자로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "321d3f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.349520Z",
     "iopub.status.busy": "2023-03-24T23:29:41.349408Z",
     "iopub.status.idle": "2023-03-24T23:29:41.396631Z",
     "shell.execute_reply": "2023-03-24T23:29:41.396167Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.349506Z"
    }
   },
   "outputs": [],
   "source": [
    "# some dependencies are Na, fill those with the square root of the square\n",
    "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
    "\n",
    "# fill \"no\"s for education with 0s\n",
    "train.loc[train['edjefa'] == \"no\", 'edjefa'] = 0\n",
    "train.loc[train['edjefe'] == \"no\", 'edjefe'] = 0\n",
    "test.loc[test['edjefa'] == \"no\", 'edjefa'] = 0\n",
    "test.loc[test['edjefe'] == \"no\", 'edjefe'] = 0\n",
    "\n",
    "# if education is \"yes\" and person is head of household, fill with escolari\n",
    "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), 'edhefa'] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), 'escolari']\n",
    "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), 'edhefe'] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), 'escolari']\n",
    "\n",
    "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), 'edhefa'] = test.loc[(train['edjefa'] == \"yes\") & (test['parentesco1'] == 1), 'escolari']\n",
    "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), 'edhefe'] = test.loc[(train['edjefe'] == \"yes\") & (test['parentesco1'] == 1), 'escolari']\n",
    "\n",
    "# this field is supposed th be interaction between gender and escolari, but it isn't clear what \"yes\" means, let's fill it with 4\n",
    "train.loc[train['edjefa'] == \"yes\", 'edjefa'] = 4\n",
    "train.loc[train['edjefe'] == \"yes\", 'edjefe'] = 4\n",
    "\n",
    "test.loc[test['edjefa'] == \"yes\", 'edjefa'] = 4\n",
    "test.loc[test['edjefe'] == \"yes\", 'edjefe'] = 4\n",
    "\n",
    "# convert to int for our models\n",
    "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
    "train['edjefe']=  train['edjefe'].astype(\"int\")\n",
    "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
    "test['edjefe']=  test['edjefe'].astype(\"int\")\n",
    "\n",
    "# create feature with max education of either head of household\n",
    "train['edjef'] = np.max(train[['edjefa', 'edjefe']], axis=1)\n",
    "test['edjef'] = np.max(train[['edjefa', 'edjefe']], axis=1)\n",
    "\n",
    "# fill some nas\n",
    "train['v2a1'] = train['v2a1'].fillna(0)\n",
    "test['v2a1'] = test['v2a1'].fillna(0)\n",
    "\n",
    "train['v18q1'] = train['v18q1'].fillna(0)\n",
    "test['v18q1'] = train['v18q1'].fillna(0)\n",
    "\n",
    "train['rez_esc'] = train['rez_esc'].fillna(0)\n",
    "test['rez_esc'] = test['rez_esc'].fillna(0)\n",
    "\n",
    "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "# fix some inconsistencies in the data - some rows indicate both the household does and does not have a totilet.\n",
    "# it the is no water we'll assume they do not\n",
    "train.loc[(train.v14a == 1) & (train.sanitario1 == 1) & (train.abastaguano == 0), \"v14a\"] = 0\n",
    "train.loc[(train.v14a == 1) & (train.sanitario1 == 1) & (train.abastaguano == 0), \"sanitario1\" ] = 0\n",
    "\n",
    "test.loc[(test.v14a == 1) & (test.sanitario1 == 1) & (test.abastaguano == 0), \"v14a\"] = 0\n",
    "test.loc[(test.v14a == 1) & (test.sanitario1 == 1) & (test.abastaguano == 0), \"sanitario1\" ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "352648a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.397225Z",
     "iopub.status.busy": "2023-03-24T23:29:41.397129Z",
     "iopub.status.idle": "2023-03-24T23:29:41.399656Z",
     "shell.execute_reply": "2023-03-24T23:29:41.399342Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.397217Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "\n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
    "\n",
    "    del xx, xx_func\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13152224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.400210Z",
     "iopub.status.busy": "2023-03-24T23:29:41.400125Z",
     "iopub.status.idle": "2023-03-24T23:29:41.716415Z",
     "shell.execute_reply": "2023-03-24T23:29:41.716053Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.400202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OHE in techo is incomplete. A new column will be added before label encoding\n",
      "The OHE in instlevel is incomplete. A new column will be added before label encoding\n",
      "The OHE in manual_elec is incomplete. A new column will be added before label encoding\n"
     ]
    }
   ],
   "source": [
    "# convert the one hot field into label encoded\n",
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e2e7d-8f9f-4a3e-9421-fac42997b602",
   "metadata": {},
   "source": [
    "# Geo aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afeb389c-38fd-4446-b470-c7e6f7b54435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.716989Z",
     "iopub.status.busy": "2023-03-24T23:29:41.716899Z",
     "iopub.status.idle": "2023-03-24T23:29:41.757971Z",
     "shell.execute_reply": "2023-03-24T23:29:41.757481Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.716981Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', \n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n",
    "                        pd.get_dummies(df_[cols_2_ohe], \n",
    "                                       columns=cols_2_ohe)],axis=1)\n",
    "\n",
    "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n",
    "    \n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "# add some aggregates by geography\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df0b6e56-dece-45a1-a02b-adb39801ba2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.758532Z",
     "iopub.status.busy": "2023-03-24T23:29:41.758435Z",
     "iopub.status.idle": "2023-03-24T23:29:41.786560Z",
     "shell.execute_reply": "2023-03-24T23:29:41.786100Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.758524Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add the number of people over 18 in each household\n",
    "train['num_over_18'] = 0\n",
    "train['num_over_18'] = train[train.age >= 18].groupby('idhogar').idhogar.transform(\"count\")\n",
    "train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "train['num_over_18'] = train['num_over_18'].fillna(0)\n",
    "\n",
    "test['num_over_18'] = 0\n",
    "test['num_over_18'] = test[test.age >= 18].groupby('idhogar').idhogar.transform(\"count\")\n",
    "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "test['num_over_18'] = test['num_over_18'].fillna(0)\n",
    "\n",
    "# add some extra features, these were taken from another kernel\n",
    "def extract_features(df):\n",
    "    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n",
    "    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n",
    "    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n",
    "    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n",
    "    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n",
    "    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n",
    "    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n",
    "    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n",
    "    # some households have no one over 18, use the total rent for those\n",
    "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n",
    "    \n",
    "extract_features(train)    \n",
    "extract_features(test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4a0bf61-8225-4df4-baa1-7920fe926603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.787405Z",
     "iopub.status.busy": "2023-03-24T23:29:41.787277Z",
     "iopub.status.idle": "2023-03-24T23:29:41.794018Z",
     "shell.execute_reply": "2023-03-24T23:29:41.793671Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.787393Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop duplicated columns\n",
    "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
    "                 'mobilephone', 'female', ]\n",
    "\n",
    "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n",
    "\n",
    "needless_cols.extend(instlevel_cols)\n",
    "\n",
    "train = train.drop(needless_cols, axis=1)\n",
    "test = test.drop(needless_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc0c891-f93a-4854-af42-89814bf4501e",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a95af-ccc9-4850-9643-da2dfda9079d",
   "metadata": {
    "tags": []
   },
   "source": [
    "같은 가구에 속한 행은 일반적으로 동일한 타겟을 갖기 때문에 누출을 방지하기 위해 데이터를 가구별로 분할합니다. 세대주만 포함하도록 데이터를 필터링하기 때문에 기술적으로 반드시 필요한 것은 아니지만, 그렇게 하려는 경우 전체 학습 데이터 세트를 쉽게 사용할 수 있는 방법을 제공합니다.\n",
    "\n",
    "데이터를 분할한 후에는 전체 데이터 세트로 학습 데이터를 덮어쓰므로 모든 데이터에 대해 학습할 수 있습니다. split_data 함수는 데이터를 덮어쓰지 않고 동일한 작업을 수행하며, 훈련 루프 내에서 (희망적으로) K-Fold 분할을 근사화하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd743e4-c58f-4e20-9922-595b008d8c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.794653Z",
     "iopub.status.busy": "2023-03-24T23:29:41.794547Z",
     "iopub.status.idle": "2023-03-24T23:29:41.797417Z",
     "shell.execute_reply": "2023-03-24T23:29:41.797136Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.794642Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(train, y, sample_weight=None, households=None, test_percentage=0.20, seed=None):\n",
    "    # uncomment for extra randomness\n",
    "    # np.random.seed(seed=seed)\n",
    "    \n",
    "    train2 = train.copy()\n",
    "    \n",
    "    # pick some random households to use for the test data\n",
    "    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n",
    "    \n",
    "    # select households which are in the random selection\n",
    "    cv_idx = np.isin(households, cv_hhs)\n",
    "    X_test = train2[cv_idx]\n",
    "    y_test = y[cv_idx]\n",
    "\n",
    "    X_train = train2[~cv_idx]\n",
    "    y_train = y[~cv_idx]\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        y_train_weights = sample_weight[~cv_idx]\n",
    "        return X_train, y_train, X_test, y_test, y_train_weights\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7677d9f-bc6a-4e83-9cac-7edfb3dd9f1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.800209Z",
     "iopub.status.busy": "2023-03-24T23:29:41.800094Z",
     "iopub.status.idle": "2023-03-24T23:29:41.809057Z",
     "shell.execute_reply": "2023-03-24T23:29:41.808799Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.800199Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = train.query('parentesco1==1')\n",
    "# X = train.copy()\n",
    "\n",
    "# pull out and drop the target variable\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'], axis=1)\n",
    "\n",
    "np.random.seed(seed=None)\n",
    "\n",
    "train2 = X.copy()\n",
    "\n",
    "train_hhs = train2.idhogar\n",
    "\n",
    "households = train2.idhogar.unique()\n",
    "cv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False)\n",
    "\n",
    "cv_idx = np.isin(train2.idhogar, cv_hhs)\n",
    "\n",
    "X_test = train2[cv_idx]\n",
    "y_test = y[cv_idx]\n",
    "\n",
    "X_train = train2[~cv_idx]\n",
    "y_train = y[~cv_idx]\n",
    "\n",
    "# train on entire dataset\n",
    "X_train = train2\n",
    "y_train = y\n",
    "\n",
    "train_households = X_train.idhogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9bb9d20-790b-4a2c-bba6-c8fefa9079e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.809541Z",
     "iopub.status.busy": "2023-03-24T23:29:41.809455Z",
     "iopub.status.idle": "2023-03-24T23:29:41.812727Z",
     "shell.execute_reply": "2023-03-24T23:29:41.812349Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.809533Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# figure out the class weights for training with unbalanced classes\n",
    "y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cd99662-1e50-4913-a756-bc8a81c02269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.813463Z",
     "iopub.status.busy": "2023-03-24T23:29:41.813345Z",
     "iopub.status.idle": "2023-03-24T23:29:41.815785Z",
     "shell.execute_reply": "2023-03-24T23:29:41.815473Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.813454Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop some features which aren't used by the LGBM or have very low importance\n",
    "extra_drop_features = [\n",
    " 'agg18_estadocivil1_MEAN',\n",
    " 'agg18_estadocivil6_COUNT',\n",
    " 'agg18_estadocivil7_COUNT',\n",
    " 'agg18_parentesco10_COUNT',\n",
    " 'agg18_parentesco11_COUNT',\n",
    " 'agg18_parentesco12_COUNT',\n",
    " 'agg18_parentesco1_COUNT',\n",
    " 'agg18_parentesco2_COUNT',\n",
    " 'agg18_parentesco3_COUNT',\n",
    " 'agg18_parentesco4_COUNT',\n",
    " 'agg18_parentesco5_COUNT',\n",
    " 'agg18_parentesco6_COUNT',\n",
    " 'agg18_parentesco7_COUNT',\n",
    " 'agg18_parentesco8_COUNT',\n",
    " 'agg18_parentesco9_COUNT',\n",
    " 'geo_elimbasu_LE_4',\n",
    " 'geo_energcocinar_LE_1',\n",
    " 'geo_energcocinar_LE_2',\n",
    " 'geo_epared_LE_0',\n",
    " 'geo_hogar_mayor',\n",
    " 'geo_manual_elec_LE_2',\n",
    " 'geo_pared_LE_3',\n",
    " 'geo_pared_LE_4',\n",
    " 'geo_pared_LE_5',\n",
    " 'geo_pared_LE_6',\n",
    " 'num_over_18',\n",
    " 'parentesco_LE',\n",
    " 'rez_esc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13fa2b5f-5bb5-4f68-bab2-82579c6a30fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.816246Z",
     "iopub.status.busy": "2023-03-24T23:29:41.816155Z",
     "iopub.status.idle": "2023-03-24T23:29:41.818050Z",
     "shell.execute_reply": "2023-03-24T23:29:41.817708Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.816239Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_drop_cols = extra_drop_features + [\"idhogar\",  'parentesco1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ea3c1-ecff-4235-84a2-71a848fe849d",
   "metadata": {},
   "source": [
    "# Fit a voting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d3bbd-e57d-4a5f-b7f9-681e24aab5f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "조기 중지를 위해 fit_params를 전달할 수 있도록 파생된 투표 분류기 클래스를 정의합니다. 매크로 F1과 감쇠 학습률을 기반으로 조기 중지를 사용하는 LGBM 모델을 기반으로 투표합니다.\n",
    "\n",
    "파라미터는 이 커널에서 무작위 검색을 통해 최적화됩니다: https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99c028ce-ba0c-412c-8234-dca7cd332ab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.818546Z",
     "iopub.status.busy": "2023-03-24T23:29:41.818464Z",
     "iopub.status.idle": "2023-03-24T23:29:41.822142Z",
     "shell.execute_reply": "2023-03-24T23:29:41.821846Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.818538Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4\n",
    "opt_parameters = {'max_depth':35, 'eta':0.1, 'verbosity':0, 'objective':'multi:softmax', 'min_child_weight': 1, 'num_class': 4, 'gamma': 2.0, 'colsample_bylevel': 0.9, 'subsample': 0.84, 'colsample_bytree': 0.88, 'reg_lambda': 0.40 }\n",
    "# 5\n",
    "opt_parameters = {'max_depth':35, 'eta':0.15, 'verbosity':1, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
    "# 6\n",
    "# opt_parameters = {'max_depth':35, 'eta':0.15, 'verbosity':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.75, 'colsample_bylevel': 0.95, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
    "# # 7\n",
    "# opt_parameters = {'max_depth':35, 'eta':0.12, 'verbosity':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 3.25, 'colsample_bylevel': 0.95, 'subsample': 0.88, 'colsample_bytree': 0.88, 'reg_lambda': 0.35 }\n",
    "\n",
    "def evaluate_macroF1_lgb(predictions, truth):  \n",
    "    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n",
    "    pred_labels = predictions.argmax()\n",
    "    truth = truth.get_label()\n",
    "    \n",
    "    f1 = f1_score(truth, predictions, average='macro')\n",
    "    return ('macroF1', 1-f1) \n",
    "\n",
    "fit_params={\"early_stopping_rounds\":500,\n",
    "            \"eval_metric\" : evaluate_macroF1_lgb, \n",
    "            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n",
    "            'verbose': False,\n",
    "           }\n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['verbose'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a485754-a585-4e0e-9f50-28e3b364a539",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.822785Z",
     "iopub.status.busy": "2023-03-24T23:29:41.822649Z",
     "iopub.status.idle": "2023-03-24T23:29:41.830655Z",
     "shell.execute_reply": "2023-03-24T23:29:41.830283Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.822776Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "    estimator = clone(estimator1)\n",
    "    \n",
    "    # randomly split the data so we have a test set for early stopping\n",
    "    if sample_weight is not None:\n",
    "        X_train, y_train, X_test, y_test, y_train_weight = split_data(X, y, sample_weight, households=train_households)\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = split_data(X, y, None, households=train_households)\n",
    "        \n",
    "    # update the fit params with our new split\n",
    "    fit_params[\"eval_set\"] = [(X_test,y_test)]\n",
    "    \n",
    "    # fit the estimator\n",
    "    if sample_weight is not None:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n",
    "    else:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
    "    \n",
    "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
    "        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
    "    else:\n",
    "        best_train = f1_score(y_train, estimator.predict(X_train), average=\"macro\")\n",
    "        best_cv = f1_score(y_test, estimator.predict(X_test), average=\"macro\")\n",
    "        print(\"Train F1:\", best_train)\n",
    "        print(\"Test F1:\", best_cv)\n",
    "        \n",
    "    # reject some estimators based on their performance on train and test sets\n",
    "    if threshold:\n",
    "        # if the valid score is very high we'll allow a little more leeway with the train scores\n",
    "        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n",
    "            return estimator\n",
    "\n",
    "        # else recurse until we get a better one\n",
    "        else:\n",
    "            print(\"Unacceptable!!! Trying again...\")\n",
    "            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n",
    "    \n",
    "    else:\n",
    "        return estimator\n",
    "    \n",
    "class VotingClassifierLGBM(VotingClassifier):\n",
    "    '''\n",
    "    This implements the fit method of the VotingClassifier propagating fit_params\n",
    "    '''\n",
    "    def fit(self, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "        \n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output'\n",
    "                                      ' classification is not supported.')\n",
    "\n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
    "                             % self.voting)\n",
    "\n",
    "        if self.estimators is None or len(self.estimators) == 0:\n",
    "            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n",
    "                                 ' should be a list of (string, estimator)'\n",
    "                                 ' tuples')\n",
    "\n",
    "        if (self.weights is not None and\n",
    "                len(self.weights) != len(self.estimators)):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d estimators'\n",
    "                             % (len(self.weights), len(self.estimators)))\n",
    "\n",
    "        names, clfs = zip(*self.estimators)\n",
    "        self._validate_names(names)\n",
    "\n",
    "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
    "        if n_isnone == len(self.estimators):\n",
    "            raise ValueError('All estimators are None. At least one is '\n",
    "                             'required to be a classifier!')\n",
    "\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.estimators_ = []\n",
    "\n",
    "        transformed_y = self.le_.transform(y)\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n",
    "                                                 sample_weight=sample_weight, threshold=threshold, **fit_params)\n",
    "                for clf in clfs if clf is not None)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d18bea2-873f-4d70-b440-b2ce36d6e298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:29:41.831223Z",
     "iopub.status.busy": "2023-03-24T23:29:41.831142Z",
     "iopub.status.idle": "2023-03-24T23:32:09.505292Z",
     "shell.execute_reply": "2023-03-24T23:32:09.504838Z",
     "shell.execute_reply.started": "2023-03-24T23:29:41.831215Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.30152\tvalidation_0-macroF1:0.65445\n",
      "[50]\tvalidation_0-mlogloss:0.91929\tvalidation_0-macroF1:0.56136\n",
      "[100]\tvalidation_0-mlogloss:0.91704\tvalidation_0-macroF1:0.56791\n",
      "[150]\tvalidation_0-mlogloss:0.91598\tvalidation_0-macroF1:0.56757\n",
      "[200]\tvalidation_0-mlogloss:0.91414\tvalidation_0-macroF1:0.56898\n",
      "[250]\tvalidation_0-mlogloss:0.91364\tvalidation_0-macroF1:0.57355\n",
      "[299]\tvalidation_0-mlogloss:0.91400\tvalidation_0-macroF1:0.57779\n",
      "Train F1: 0.8935168893176608\n",
      "Test F1: 0.44551431974757316\n",
      "[0]\tvalidation_0-mlogloss:1.31088\tvalidation_0-macroF1:0.65553\n",
      "[50]\tvalidation_0-mlogloss:0.91842\tvalidation_0-macroF1:0.56830\n",
      "[100]\tvalidation_0-mlogloss:0.91848\tvalidation_0-macroF1:0.59189\n",
      "[150]\tvalidation_0-mlogloss:0.91699\tvalidation_0-macroF1:0.57406\n",
      "[200]\tvalidation_0-mlogloss:0.91389\tvalidation_0-macroF1:0.58228\n",
      "[250]\tvalidation_0-mlogloss:0.91314\tvalidation_0-macroF1:0.58592\n",
      "[299]\tvalidation_0-mlogloss:0.91198\tvalidation_0-macroF1:0.58296\n",
      "Train F1: 0.8977184399356206\n",
      "Test F1: 0.43775484996370084\n",
      "[0]\tvalidation_0-mlogloss:1.29921\tvalidation_0-macroF1:0.62563\n",
      "[50]\tvalidation_0-mlogloss:0.94419\tvalidation_0-macroF1:0.59254\n",
      "[100]\tvalidation_0-mlogloss:0.93788\tvalidation_0-macroF1:0.59255\n",
      "[150]\tvalidation_0-mlogloss:0.93335\tvalidation_0-macroF1:0.58573\n",
      "[200]\tvalidation_0-mlogloss:0.93197\tvalidation_0-macroF1:0.58924\n",
      "[250]\tvalidation_0-mlogloss:0.93116\tvalidation_0-macroF1:0.59121\n",
      "[299]\tvalidation_0-mlogloss:0.92911\tvalidation_0-macroF1:0.58961\n",
      "Train F1: 0.8945825582231971\n",
      "Test F1: 0.42380684149594067\n",
      "[0]\tvalidation_0-mlogloss:1.30800\tvalidation_0-macroF1:0.64808\n",
      "[50]\tvalidation_0-mlogloss:0.92753\tvalidation_0-macroF1:0.57798\n",
      "[100]\tvalidation_0-mlogloss:0.92264\tvalidation_0-macroF1:0.58275\n",
      "[150]\tvalidation_0-mlogloss:0.92387\tvalidation_0-macroF1:0.56671\n",
      "[200]\tvalidation_0-mlogloss:0.92337\tvalidation_0-macroF1:0.57179\n",
      "[250]\tvalidation_0-mlogloss:0.92143\tvalidation_0-macroF1:0.57586\n",
      "[299]\tvalidation_0-mlogloss:0.92117\tvalidation_0-macroF1:0.58161\n",
      "Train F1: 0.9146036305424045\n",
      "Test F1: 0.43329245235348457\n",
      "[0]\tvalidation_0-mlogloss:1.30316\tvalidation_0-macroF1:0.64375\n",
      "[50]\tvalidation_0-mlogloss:0.93375\tvalidation_0-macroF1:0.60040\n",
      "[100]\tvalidation_0-mlogloss:0.92897\tvalidation_0-macroF1:0.60667\n",
      "[150]\tvalidation_0-mlogloss:0.92728\tvalidation_0-macroF1:0.61047\n",
      "[200]\tvalidation_0-mlogloss:0.92780\tvalidation_0-macroF1:0.60468\n",
      "[250]\tvalidation_0-mlogloss:0.92926\tvalidation_0-macroF1:0.60305\n",
      "[299]\tvalidation_0-mlogloss:0.92879\tvalidation_0-macroF1:0.60170\n",
      "Train F1: 0.895986830363007\n",
      "Test F1: 0.4343009278173186\n",
      "[0]\tvalidation_0-mlogloss:1.30212\tvalidation_0-macroF1:0.64533\n",
      "[50]\tvalidation_0-mlogloss:0.91346\tvalidation_0-macroF1:0.59054\n",
      "[100]\tvalidation_0-mlogloss:0.91114\tvalidation_0-macroF1:0.58992\n",
      "[150]\tvalidation_0-mlogloss:0.90684\tvalidation_0-macroF1:0.58674\n",
      "[200]\tvalidation_0-mlogloss:0.90922\tvalidation_0-macroF1:0.59025\n",
      "[250]\tvalidation_0-mlogloss:0.90757\tvalidation_0-macroF1:0.59174\n",
      "[299]\tvalidation_0-mlogloss:0.90577\tvalidation_0-macroF1:0.59061\n",
      "Train F1: 0.9131795306463479\n",
      "Test F1: 0.4310330357737885\n",
      "[0]\tvalidation_0-mlogloss:1.30607\tvalidation_0-macroF1:0.64045\n",
      "[50]\tvalidation_0-mlogloss:0.91092\tvalidation_0-macroF1:0.60532\n",
      "[100]\tvalidation_0-mlogloss:0.90514\tvalidation_0-macroF1:0.59827\n",
      "[150]\tvalidation_0-mlogloss:0.90589\tvalidation_0-macroF1:0.60384\n",
      "[200]\tvalidation_0-mlogloss:0.90590\tvalidation_0-macroF1:0.59395\n",
      "[250]\tvalidation_0-mlogloss:0.90542\tvalidation_0-macroF1:0.59687\n",
      "[299]\tvalidation_0-mlogloss:0.90776\tvalidation_0-macroF1:0.59981\n",
      "Train F1: 0.9284544598489382\n",
      "Test F1: 0.4064924830077692\n",
      "[0]\tvalidation_0-mlogloss:1.30613\tvalidation_0-macroF1:0.67849\n",
      "[50]\tvalidation_0-mlogloss:0.93358\tvalidation_0-macroF1:0.61791\n",
      "[100]\tvalidation_0-mlogloss:0.93114\tvalidation_0-macroF1:0.61586\n",
      "[150]\tvalidation_0-mlogloss:0.92682\tvalidation_0-macroF1:0.61501\n",
      "[200]\tvalidation_0-mlogloss:0.92765\tvalidation_0-macroF1:0.60162\n",
      "[250]\tvalidation_0-mlogloss:0.92777\tvalidation_0-macroF1:0.60409\n",
      "[299]\tvalidation_0-mlogloss:0.92853\tvalidation_0-macroF1:0.60734\n",
      "Train F1: 0.9296299431562589\n",
      "Test F1: 0.4025903390443083\n",
      "[0]\tvalidation_0-mlogloss:1.29265\tvalidation_0-macroF1:0.59502\n",
      "[50]\tvalidation_0-mlogloss:0.86333\tvalidation_0-macroF1:0.53064\n",
      "[100]\tvalidation_0-mlogloss:0.85400\tvalidation_0-macroF1:0.52034\n",
      "[150]\tvalidation_0-mlogloss:0.85128\tvalidation_0-macroF1:0.51512\n",
      "[200]\tvalidation_0-mlogloss:0.85214\tvalidation_0-macroF1:0.50245\n",
      "[250]\tvalidation_0-mlogloss:0.85185\tvalidation_0-macroF1:0.51290\n",
      "[299]\tvalidation_0-mlogloss:0.85075\tvalidation_0-macroF1:0.50954\n",
      "Train F1: 0.9269983877758698\n",
      "Test F1: 0.5005574252639174\n",
      "[0]\tvalidation_0-mlogloss:1.29443\tvalidation_0-macroF1:0.63673\n",
      "[50]\tvalidation_0-mlogloss:0.89602\tvalidation_0-macroF1:0.62213\n",
      "[100]\tvalidation_0-mlogloss:0.88513\tvalidation_0-macroF1:0.61396\n",
      "[150]\tvalidation_0-mlogloss:0.88316\tvalidation_0-macroF1:0.61985\n",
      "[200]\tvalidation_0-mlogloss:0.88496\tvalidation_0-macroF1:0.61744\n",
      "[250]\tvalidation_0-mlogloss:0.88283\tvalidation_0-macroF1:0.61871\n",
      "[299]\tvalidation_0-mlogloss:0.88221\tvalidation_0-macroF1:0.62174\n",
      "Train F1: 0.8183129475946853\n",
      "Test F1: 0.39855126546176034\n",
      "[0]\tvalidation_0-mlogloss:1.29737\tvalidation_0-macroF1:0.63187\n",
      "[50]\tvalidation_0-mlogloss:0.93159\tvalidation_0-macroF1:0.58736\n",
      "[100]\tvalidation_0-mlogloss:0.93075\tvalidation_0-macroF1:0.58597\n",
      "[150]\tvalidation_0-mlogloss:0.92926\tvalidation_0-macroF1:0.57931\n",
      "[200]\tvalidation_0-mlogloss:0.93001\tvalidation_0-macroF1:0.59086\n",
      "[250]\tvalidation_0-mlogloss:0.93120\tvalidation_0-macroF1:0.59591\n",
      "[299]\tvalidation_0-mlogloss:0.93009\tvalidation_0-macroF1:0.59672\n",
      "Train F1: 0.8854634887024214\n",
      "Test F1: 0.4469433893250311\n",
      "[0]\tvalidation_0-mlogloss:1.29969\tvalidation_0-macroF1:0.63170\n",
      "[50]\tvalidation_0-mlogloss:0.93084\tvalidation_0-macroF1:0.59097\n",
      "[100]\tvalidation_0-mlogloss:0.93208\tvalidation_0-macroF1:0.59971\n",
      "[150]\tvalidation_0-mlogloss:0.93234\tvalidation_0-macroF1:0.59906\n",
      "[200]\tvalidation_0-mlogloss:0.93317\tvalidation_0-macroF1:0.60199\n",
      "[250]\tvalidation_0-mlogloss:0.93321\tvalidation_0-macroF1:0.59946\n",
      "[299]\tvalidation_0-mlogloss:0.93509\tvalidation_0-macroF1:0.59886\n",
      "Train F1: 0.8868205805514882\n",
      "Test F1: 0.4196748586326332\n",
      "[0]\tvalidation_0-mlogloss:1.30416\tvalidation_0-macroF1:0.64597\n",
      "[50]\tvalidation_0-mlogloss:0.90074\tvalidation_0-macroF1:0.56798\n",
      "[100]\tvalidation_0-mlogloss:0.89394\tvalidation_0-macroF1:0.57061\n",
      "[150]\tvalidation_0-mlogloss:0.89569\tvalidation_0-macroF1:0.57729\n",
      "[200]\tvalidation_0-mlogloss:0.89360\tvalidation_0-macroF1:0.57856\n",
      "[250]\tvalidation_0-mlogloss:0.89403\tvalidation_0-macroF1:0.58725\n",
      "[299]\tvalidation_0-mlogloss:0.89536\tvalidation_0-macroF1:0.59271\n",
      "Train F1: 0.8931171516790561\n",
      "Test F1: 0.4420477640464662\n",
      "[0]\tvalidation_0-mlogloss:1.31693\tvalidation_0-macroF1:0.66896\n",
      "[50]\tvalidation_0-mlogloss:0.93602\tvalidation_0-macroF1:0.61583\n",
      "[100]\tvalidation_0-mlogloss:0.92837\tvalidation_0-macroF1:0.60696\n",
      "[150]\tvalidation_0-mlogloss:0.92581\tvalidation_0-macroF1:0.60600\n",
      "[200]\tvalidation_0-mlogloss:0.92517\tvalidation_0-macroF1:0.60286\n",
      "[250]\tvalidation_0-mlogloss:0.92440\tvalidation_0-macroF1:0.60934\n",
      "[299]\tvalidation_0-mlogloss:0.92369\tvalidation_0-macroF1:0.61281\n",
      "Train F1: 0.9207073031026731\n",
      "Test F1: 0.4031882567778984\n",
      "[0]\tvalidation_0-mlogloss:1.29626\tvalidation_0-macroF1:0.64943\n",
      "[50]\tvalidation_0-mlogloss:0.87261\tvalidation_0-macroF1:0.56502\n",
      "[100]\tvalidation_0-mlogloss:0.86640\tvalidation_0-macroF1:0.55693\n",
      "[150]\tvalidation_0-mlogloss:0.86539\tvalidation_0-macroF1:0.55755\n",
      "[200]\tvalidation_0-mlogloss:0.86134\tvalidation_0-macroF1:0.53846\n",
      "[250]\tvalidation_0-mlogloss:0.86121\tvalidation_0-macroF1:0.53848\n",
      "[299]\tvalidation_0-mlogloss:0.86273\tvalidation_0-macroF1:0.54028\n",
      "Train F1: 0.9216942000208524\n",
      "Test F1: 0.46241879760170884\n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "for i in range(15):\n",
    "    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, learning_rate=0.15, n_jobs=4, **opt_parameters)\n",
    "    \n",
    "    clfs.append(('xgb{}'.format(i), clf))\n",
    "    \n",
    "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
    "del(clfs)\n",
    "\n",
    "#Train the final model with learning rate decay\n",
    "\n",
    "_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n",
    "\n",
    "clf_final = vc.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc1c2e63-5c7a-4bd8-a2dc-b4ca137ed80d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:09.505974Z",
     "iopub.status.busy": "2023-03-24T23:32:09.505877Z",
     "iopub.status.idle": "2023-03-24T23:32:09.633693Z",
     "shell.execute_reply": "2023-03-24T23:32:09.633238Z",
     "shell.execute_reply.started": "2023-03-24T23:32:09.505965Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a single LGBM Classifier: 0.8398\n",
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.9208\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.9156\n"
     ]
    }
   ],
   "source": [
    "# params 4 - 400 early stop - 15 estimators - l1 used features - weighted\n",
    "global_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "vc.voting = 'soft'\n",
    "global_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "vc.voting = 'hard'\n",
    "global_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee638933-6698-4073-82bc-d8409fd57de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:09.634403Z",
     "iopub.status.busy": "2023-03-24T23:32:09.634302Z",
     "iopub.status.idle": "2023-03-24T23:32:09.658796Z",
     "shell.execute_reply": "2023-03-24T23:32:09.658379Z",
     "shell.execute_reply.started": "2023-03-24T23:32:09.634394Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agg18_estadocivil5_COUNT', 'geo_energcocinar_LE_0', 'geo_manual_elec_LE_3'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see which features are not used by ANY models\n",
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83a238dc-276e-4e39-acd5-37ae947ab30a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:09.659514Z",
     "iopub.status.busy": "2023-03-24T23:32:09.659404Z",
     "iopub.status.idle": "2023-03-24T23:32:09.664390Z",
     "shell.execute_reply": "2023-03-24T23:32:09.663855Z",
     "shell.execute_reply.started": "2023-03-24T23:32:09.659506Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 59 (0.021402) - agg18_escolari_MAX\n",
      "2. feature 42 (0.018929) - fe_children_fraction\n",
      "3. feature 133 (0.018828) - geo_manual_elec_LE_4\n",
      "4. feature 40 (0.015555) - SQBdependency\n",
      "5. feature 74 (0.015502) - agg18_parentesco2_MEAN\n",
      "6. feature 86 (0.012092) - edhefe\n",
      "7. feature 37 (0.011678) - SQBedjefe\n",
      "8. feature 60 (0.011528) - agg18_escolari_MEAN\n",
      "9. feature 126 (0.011523) - geo_sanitario_LE_1\n",
      "10. feature 22 (0.011509) - dependency\n",
      "11. feature 122 (0.010849) - geo_elimbasu_LE_5\n",
      "12. feature 96 (0.010796) - etecho_LE\n",
      "13. feature 108 (0.010728) - geo_bedrooms\n",
      "14. feature 103 (0.010532) - geo_meaneduc\n",
      "15. feature 13 (0.010358) - r4t2\n",
      "16. feature 12 (0.010064) - r4t1\n",
      "17. feature 17 (0.010061) - male\n",
      "18. feature 89 (0.009666) - piso_LE\n",
      "19. feature 35 (0.009661) - SQBage\n",
      "20. feature 15 (0.009604) - cielorazo\n",
      "21. feature 97 (0.009582) - eviv_LE\n",
      "22. feature 19 (0.009575) - hogar_adul\n",
      "23. feature 1 (0.009529) - hacdor\n",
      "24. feature 116 (0.009529) - geo_epared_LE_1\n",
      "25. feature 95 (0.009451) - epared_LE\n",
      "26. feature 49 (0.009413) - fe_mobile_density\n",
      "27. feature 14 (0.009381) - escolari\n",
      "28. feature 63 (0.009333) - agg18_estadocivil2_MEAN\n",
      "29. feature 99 (0.009229) - lugar_LE\n",
      "30. feature 23 (0.009192) - edjefe\n",
      "31. feature 27 (0.009103) - overcrowding\n",
      "32. feature 100 (0.009100) - tipovivi_LE\n",
      "33. feature 109 (0.009099) - geo_overcrowding\n",
      "34. feature 136 (0.009046) - geo_pared_LE_2\n",
      "35. feature 98 (0.009029) - estadocivil_LE\n",
      "36. feature 88 (0.009016) - pared_LE\n",
      "37. feature 58 (0.009011) - agg18_escolari_MIN\n",
      "38. feature 55 (0.008909) - agg18_age_MIN\n",
      "39. feature 111 (0.008904) - geo_eviv_LE_1\n",
      "40. feature 107 (0.008846) - geo_hogar_total\n",
      "41. feature 114 (0.008843) - geo_etecho_LE_1\n",
      "42. feature 7 (0.008829) - r4h2\n",
      "43. feature 33 (0.008616) - age\n",
      "44. feature 71 (0.008550) - agg18_estadocivil6_MEAN\n",
      "45. feature 65 (0.008476) - agg18_estadocivil3_MEAN\n",
      "46. feature 130 (0.008445) - geo_manual_elec_LE_0\n",
      "47. feature 124 (0.008416) - geo_energcocinar_LE_3\n",
      "48. feature 106 (0.008337) - geo_hogar_adul\n",
      "49. feature 0 (0.008285) - v2a1\n",
      "50. feature 118 (0.008266) - geo_elimbasu_LE_0\n",
      "51. feature 43 (0.008170) - fe_working_man_fraction\n",
      "52. feature 41 (0.008062) - SQBmeaned\n",
      "53. feature 104 (0.008031) - geo_dependency\n",
      "54. feature 44 (0.007988) - fe_all_man_fraction\n",
      "55. feature 69 (0.007958) - agg18_estadocivil5_MEAN\n",
      "56. feature 51 (0.007944) - fe_mobile_adult_density\n",
      "57. feature 30 (0.007943) - qmobilephone\n",
      "58. feature 36 (0.007893) - SQBhogar_total\n",
      "59. feature 21 (0.007880) - hogar_total\n",
      "60. feature 94 (0.007846) - elimbasu_LE\n",
      "61. feature 11 (0.007828) - r4m3\n",
      "62. feature 47 (0.007825) - fe_rent_per_person\n",
      "63. feature 61 (0.007807) - agg18_dis_MEAN\n",
      "64. feature 25 (0.007721) - meaneduc\n",
      "65. feature 138 (0.007719) - bedrooms_to_rooms\n",
      "66. feature 72 (0.007662) - agg18_estadocivil7_MEAN\n",
      "67. feature 87 (0.007650) - edjef\n",
      "68. feature 139 (0.007606) - rent_to_rooms\n",
      "69. feature 34 (0.007603) - SQBescolari\n",
      "70. feature 62 (0.007544) - agg18_estadocivil1_COUNT\n",
      "71. feature 5 (0.007535) - v18q1\n",
      "72. feature 56 (0.007500) - agg18_age_MAX\n",
      "73. feature 16 (0.007446) - dis\n",
      "74. feature 102 (0.007431) - geo_age\n",
      "75. feature 29 (0.007413) - television\n",
      "76. feature 26 (0.007402) - bedrooms\n",
      "77. feature 45 (0.007349) - fe_human_density\n",
      "78. feature 6 (0.007278) - r4h1\n",
      "79. feature 2 (0.007223) - rooms\n",
      "80. feature 92 (0.007214) - sanitario_LE\n",
      "81. feature 93 (0.007208) - energcocinar_LE\n",
      "82. feature 50 (0.007192) - fe_tablet_density\n",
      "83. feature 8 (0.007159) - r4h3\n",
      "84. feature 57 (0.007104) - agg18_age_MEAN\n",
      "85. feature 39 (0.007046) - SQBovercrowding\n",
      "86. feature 3 (0.006905) - hacapo\n",
      "87. feature 119 (0.006885) - geo_elimbasu_LE_1\n",
      "88. feature 90 (0.006838) - techo_LE\n",
      "89. feature 31 (0.006678) - area1\n",
      "90. feature 145 (0.006639) - rent_to_hhsize\n",
      "91. feature 140 (0.006528) - tamhog_to_rooms\n",
      "92. feature 9 (0.006488) - r4m1\n",
      "93. feature 10 (0.006484) - r4m2\n",
      "94. feature 24 (0.006469) - edjefa\n",
      "95. feature 142 (0.006443) - r4t3_to_rooms\n",
      "96. feature 48 (0.006354) - fe_rent_per_room\n",
      "97. feature 18 (0.006350) - hogar_nin\n",
      "98. feature 52 (0.006165) - fe_tablet_adult_density\n",
      "99. feature 38 (0.006101) - SQBhogar_nin\n",
      "100. feature 83 (0.005979) - agg18_parentesco11_MEAN\n",
      "101. feature 129 (0.005770) - geo_sanitario_LE_4\n",
      "102. feature 67 (0.005749) - agg18_estadocivil4_MEAN\n",
      "103. feature 75 (0.005682) - agg18_parentesco3_MEAN\n",
      "104. feature 112 (0.005647) - geo_eviv_LE_2\n",
      "105. feature 101 (0.005603) - manual_elec_LE\n",
      "106. feature 4 (0.005444) - refrig\n",
      "107. feature 64 (0.005402) - agg18_estadocivil2_COUNT\n",
      "108. feature 77 (0.005370) - agg18_parentesco5_MEAN\n",
      "109. feature 143 (0.004896) - v2a1_to_r4t3\n",
      "110. feature 144 (0.004887) - hhsize_to_rooms\n",
      "111. feature 128 (0.004777) - geo_sanitario_LE_3\n",
      "112. feature 146 (0.004731) - rent_to_over_18\n",
      "113. feature 105 (0.004462) - geo_hogar_nin\n",
      "114. feature 20 (0.004397) - hogar_mayor\n",
      "115. feature 28 (0.004347) - computer\n",
      "116. feature 53 (0.004225) - fe_people_not_living\n",
      "117. feature 78 (0.004196) - agg18_parentesco6_MEAN\n",
      "118. feature 32 (0.004191) - area2\n",
      "119. feature 81 (0.004124) - agg18_parentesco9_MEAN\n",
      "120. feature 79 (0.003863) - agg18_parentesco7_MEAN\n",
      "121. feature 91 (0.003336) - abastagua_LE\n",
      "122. feature 76 (0.003310) - agg18_parentesco4_MEAN\n",
      "123. feature 84 (0.002760) - agg18_parentesco12_MEAN\n",
      "124. feature 127 (0.002730) - geo_sanitario_LE_2\n",
      "125. feature 82 (0.002612) - agg18_parentesco10_MEAN\n",
      "126. feature 141 (0.002016) - r4t3_to_tamhog\n",
      "127. feature 80 (0.001788) - agg18_parentesco8_MEAN\n",
      "128. feature 121 (0.000000) - geo_elimbasu_LE_3\n",
      "129. feature 132 (0.000000) - geo_manual_elec_LE_3\n",
      "130. feature 134 (0.000000) - geo_pared_LE_0\n",
      "131. feature 135 (0.000000) - geo_pared_LE_1\n",
      "132. feature 131 (0.000000) - geo_manual_elec_LE_1\n",
      "133. feature 137 (0.000000) - geo_pared_LE_7\n",
      "134. feature 125 (0.000000) - geo_sanitario_LE_0\n",
      "135. feature 123 (0.000000) - geo_energcocinar_LE_0\n",
      "136. feature 46 (0.000000) - fe_human_bed_density\n",
      "137. feature 120 (0.000000) - geo_elimbasu_LE_2\n",
      "138. feature 117 (0.000000) - geo_epared_LE_2\n",
      "139. feature 115 (0.000000) - geo_etecho_LE_2\n",
      "140. feature 113 (0.000000) - geo_etecho_LE_0\n",
      "141. feature 110 (0.000000) - geo_eviv_LE_0\n",
      "142. feature 85 (0.000000) - edhefa\n",
      "143. feature 70 (0.000000) - agg18_estadocivil5_COUNT\n",
      "144. feature 68 (0.000000) - agg18_estadocivil4_COUNT\n",
      "145. feature 66 (0.000000) - agg18_estadocivil3_COUNT\n",
      "146. feature 54 (0.000000) - fe_people_weird_stat\n",
      "147. feature 73 (0.000000) - agg18_parentesco1_MEAN\n"
     ]
    }
   ],
   "source": [
    "ranked_features = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2ca6c-7e08-4c73-8524-e7b3a46d2b40",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38738b4b-9fc5-4b76-9bd1-562ad767d09a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:09.665140Z",
     "iopub.status.busy": "2023-03-24T23:32:09.664970Z",
     "iopub.status.idle": "2023-03-24T23:32:09.670820Z",
     "shell.execute_reply": "2023-03-24T23:32:09.670455Z",
     "shell.execute_reply.started": "2023-03-24T23:32:09.665117Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n",
    "       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n",
    "       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n",
    "       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n",
    "       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n",
    "       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n",
    "       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n",
    "       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n",
    "       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n",
    "       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n",
    "       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n",
    "       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n",
    "       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n",
    "       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n",
    "       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n",
    "       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n",
    "       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n",
    "       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n",
    "       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n",
    "       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n",
    "       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN'] #+ ['parentesco_LE', 'rez_esc']\n",
    "\n",
    "et_drop_cols.extend([\"idhogar\", \"parentesco1\", 'fe_rent_per_person', 'fe_rent_per_room',\n",
    "       'fe_tablet_adult_density', 'fe_tablet_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d8e4b3b-35ed-4f2f-863a-f3064a9cac05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:09.671381Z",
     "iopub.status.busy": "2023-03-24T23:32:09.671265Z",
     "iopub.status.idle": "2023-03-24T23:32:18.734035Z",
     "shell.execute_reply": "2023-03-24T23:32:18.733642Z",
     "shell.execute_reply.started": "2023-03-24T23:32:09.671372Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.8994073227039447\n",
      "Test F1: 0.4077767086983677\n",
      "Train F1: 0.8981928351735758\n",
      "Test F1: 0.4040791686810088\n",
      "Train F1: 0.8917778100487993\n",
      "Test F1: 0.4004984813375832\n",
      "Train F1: 0.8904000579200092\n",
      "Test F1: 0.4221753518260778\n",
      "Train F1: 0.8944843820452583\n",
      "Test F1: 0.4526988244247092\n",
      "Train F1: 0.8933756963570761\n",
      "Test F1: 0.4491052669031149\n",
      "Train F1: 0.8905908707876973\n",
      "Test F1: 0.4143903822720081\n",
      "Train F1: 0.8924615102721188\n",
      "Test F1: 0.46511308056118383\n",
      "Train F1: 0.9065299561598678\n",
      "Test F1: 0.4370083228479642\n",
      "Train F1: 0.8936010213926351\n",
      "Test F1: 0.42396487590364973\n"
     ]
    }
   ],
   "source": [
    "# do the same thing for some extra trees classifiers\n",
    "ets = []    \n",
    "for i in range(10):\n",
    "    rf = RandomForestClassifier(max_depth=None, random_state=217+i, n_jobs=4, n_estimators=700, min_impurity_decrease=1e-3, min_samples_leaf=2, verbose=0, class_weight=\"balanced\")\n",
    "    ets.append(('rf{}'.format(i), rf))   \n",
    "\n",
    "vc2 = VotingClassifierLGBM(ets, voting='soft')  \n",
    "_ = vc2.fit(X_train.drop(et_drop_cols, axis=1).fillna(0), y_train, threshold=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ee88297-4033-48b7-853d-1ec9d3bdbe81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:18.734746Z",
     "iopub.status.busy": "2023-03-24T23:32:18.734652Z",
     "iopub.status.idle": "2023-03-24T23:32:19.944491Z",
     "shell.execute_reply": "2023-03-24T23:32:19.944143Z",
     "shell.execute_reply.started": "2023-03-24T23:32:18.734737Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8666\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8905\n"
     ]
    }
   ],
   "source": [
    "# w/ threshold, extra drop cols\n",
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1).fillna(0)), average='macro')\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1).fillna(0)), average='macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0ea2b3b-7f89-4309-bce0-b26e08485130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:19.945141Z",
     "iopub.status.busy": "2023-03-24T23:32:19.945053Z",
     "iopub.status.idle": "2023-03-24T23:32:21.152873Z",
     "shell.execute_reply": "2023-03-24T23:32:21.152487Z",
     "shell.execute_reply.started": "2023-03-24T23:32:19.945133Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8666\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8905\n"
     ]
    }
   ],
   "source": [
    "# w/o threshold, extra drop cols\n",
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1).fillna(0)), average='macro')\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1).fillna(0)), average='macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1f672cc-61a3-47eb-a3e7-834491d4b941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:21.153490Z",
     "iopub.status.busy": "2023-03-24T23:32:21.153400Z",
     "iopub.status.idle": "2023-03-24T23:32:21.434694Z",
     "shell.execute_reply": "2023-03-24T23:32:21.434350Z",
     "shell.execute_reply.started": "2023-03-24T23:32:21.153481Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parentesco_LE', 'rez_esc'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see which features are not used by ANY models\n",
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc2.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b32003e2-4f25-435e-ab7f-1be0e95d2462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:21.435349Z",
     "iopub.status.busy": "2023-03-24T23:32:21.435258Z",
     "iopub.status.idle": "2023-03-24T23:32:21.438315Z",
     "shell.execute_reply": "2023-03-24T23:32:21.437881Z",
     "shell.execute_reply.started": "2023-03-24T23:32:21.435340Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_voters(data, weights=[0.5, 0.5]):\n",
    "    # do soft voting with both classifiers\n",
    "    vc.voting=\"soft\"\n",
    "    vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis=1))\n",
    "    vc2.voting=\"soft\"\n",
    "    vc2_probs = vc2.predict_proba(data.drop(et_drop_cols, axis=1))\n",
    "    \n",
    "    final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n",
    "    predictions = np.argmax(final_vote, axis=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a00966d6-049c-458f-8d90-43d09c75eb3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:21.438864Z",
     "iopub.status.busy": "2023-03-24T23:32:21.438780Z",
     "iopub.status.idle": "2023-03-24T23:32:22.132432Z",
     "shell.execute_reply": "2023-03-24T23:32:22.132098Z",
     "shell.execute_reply.started": "2023-03-24T23:32:21.438856Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9060685954382525"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test.fillna(0), weights=[0.5, 0.5])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec679aee-bbc6-4305-aaef-26790a65fb86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:22.133059Z",
     "iopub.status.busy": "2023-03-24T23:32:22.132971Z",
     "iopub.status.idle": "2023-03-24T23:32:22.823225Z",
     "shell.execute_reply": "2023-03-24T23:32:22.822916Z",
     "shell.execute_reply.started": "2023-03-24T23:32:22.133050Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905543594278918"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test.fillna(0), weights=[0.4, 0.6])\n",
    "global_combo_score_soft= f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98301df0-b4a9-484c-bf2d-c3c782c0e17e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:22.823744Z",
     "iopub.status.busy": "2023-03-24T23:32:22.823653Z",
     "iopub.status.idle": "2023-03-24T23:32:23.516250Z",
     "shell.execute_reply": "2023-03-24T23:32:23.515875Z",
     "shell.execute_reply.started": "2023-03-24T23:32:22.823736Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9060685954382525"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test.fillna(0), weights=[0.6, 0.4])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf44184-83c5-4ac7-9ce0-c83323f52efc",
   "metadata": {},
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c08a933c-c9b2-4a53-891a-dfc768e9330d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:23.516914Z",
     "iopub.status.busy": "2023-03-24T23:32:23.516809Z",
     "iopub.status.idle": "2023-03-24T23:32:23.520894Z",
     "shell.execute_reply": "2023-03-24T23:32:23.520480Z",
     "shell.execute_reply.started": "2023-03-24T23:32:23.516905Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_subm = pd.DataFrame()\n",
    "y_subm['Id'] = test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "134d3261-cd46-401f-9107-a61f7f3e9ff4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-24T23:32:23.521503Z",
     "iopub.status.busy": "2023-03-24T23:32:23.521384Z",
     "iopub.status.idle": "2023-03-24T23:32:29.437847Z",
     "shell.execute_reply": "2023-03-24T23:32:29.437493Z",
     "shell.execute_reply.started": "2023-03-24T23:32:23.521494Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vc.voting = 'soft'\n",
    "y_subm_lgb = y_subm.copy(deep=True)\n",
    "y_subm_lgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis=1).fillna(0)) + 1\n",
    "\n",
    "vc2.voting = 'soft'\n",
    "y_subm_rf = y_subm.copy(deep=True)\n",
    "y_subm_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis=1).fillna(0)) + 1\n",
    "\n",
    "y_subm_ens = y_subm.copy(deep=True)\n",
    "y_subm_ens['Target'] = combine_voters(test.fillna(0)) + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
